{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "526e8da7",
   "metadata": {},
   "source": [
    "# Webscraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedde284",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "232ab9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup # For HTML parsing\n",
    "import requests # Website connections\n",
    "from time import sleep # To prevent overwhelming the server between connections\n",
    "from collections import Counter # Keep track of our term counts\n",
    "import pandas as pd # For converting results to a dataframe and bar chart plots\n",
    "import json # For parsing json\n",
    "import random # For using it as random time\n",
    "import re     # For regular expressions searches within strings\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe543af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19191b96",
   "metadata": {},
   "source": [
    "## Initialize Selenium Browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03c11286",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gb/8r1vvmnj7k99ch6mg1hrh9y00000gp/T/ipykernel_38072/4019327834.py:2: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome('/Users/wk/Documents/01-Courses/9-WeCloudBootcamp/Bootcamp/2-Python/Week 2/chromedriver')\n"
     ]
    }
   ],
   "source": [
    "# Opens the Selenium browser\n",
    "driver = webdriver.Chrome('/Users/wk/Documents/01-Courses/9-WeCloudBootcamp/Bootcamp/2-Python/Week 2/chromedriver')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb97bda",
   "metadata": {},
   "source": [
    "# Write functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183804c2",
   "metadata": {},
   "source": [
    "## Function to Scrape Remax Webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26be457c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------\n",
    "# Initialize Empty Lists to Store Data as Retrived from each page\n",
    "#-----------------------------------------------------------------\n",
    "\n",
    "# Initialize empty lists to store basic information\n",
    "basic_info = []   \n",
    "\n",
    "# Initialize empty list to store detailed information data\n",
    "detail_info = []  \n",
    "\n",
    "#-----------------------------------------------------------------\n",
    "# Web Scraping Function\n",
    "#-----------------------------------------------------------------\n",
    "\n",
    "def web_scrape(province,city,num_pages):\n",
    "    '''\n",
    "    \n",
    "    This function scrapes the listing information of real estate properties from\n",
    "    REMAX website and store the into two seperate lists basic_info and detail_info lists.\n",
    "    \n",
    "    INPUTS:\n",
    "    province : Name of province in Canada such as (on, bc, ab, sk, ns, mb)\n",
    "    city     : Name of city in Canada such as (toronto, vancouver, ottawa, calgary, halifax,\n",
    "    saskatoon, edmonton, winnipeg, hamilton, surrey) \n",
    "    num_pages : Number of webpages to scrape information from.\n",
    "    \n",
    "    Note: Make sure that the city names are consitent with the province names and that maximum \n",
    "    num_pages should not exceed the actual pages on REMAX website for each city.\n",
    "    \n",
    "    OUTPUTS:\n",
    "    basic_info : List which stores basic information of houses\n",
    "    detail_info : List which stores detailed information of houses\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Start a counter to keep track of number of units scraped\n",
    "    counter = 0\n",
    "    \n",
    "    #-----------------------------------------------------------------\n",
    "    # Loop over number of pages\n",
    "    #-----------------------------------------------------------------\n",
    "    for page in range(1, num_pages+1):\n",
    "        \n",
    "        # Get the url path based on province, city and page\n",
    "        url_path = f'https://www.remax.ca/{province}/{city}-real-estate?pageNumber={page}'\n",
    "        print(f'Scraping this url now => {url_path}')\n",
    "        \n",
    "        # Go to the webpage using Selenium driver\n",
    "        driver.get(url_path)\n",
    "        \n",
    "        # Add random sleep to prevent being blocked by website\n",
    "        sleep(random.randint(2,10))\n",
    "              \n",
    "        # Find all card tags on web page using class name\n",
    "        card_tags = driver.find_elements_by_class_name('listing-card_listingCard__3SoUb')\n",
    "        \n",
    "        #-----------------------------------------------------------------    \n",
    "        # Loop over all card_tags\n",
    "        #-----------------------------------------------------------------\n",
    "        for tag in card_tags:\n",
    "            \n",
    "            # Extract text from basic information tags and append them together\n",
    "            basic_info.append(tag.text)\n",
    "            \n",
    "            # Get details information by clicking on each tag (opens a new window in a new tab)\n",
    "            #-----------------------------------------------------------------\n",
    "            \n",
    "            # Click on each listings\n",
    "            tag.click()\n",
    "            \n",
    "            # Add random sleep to prevent being blocked by website\n",
    "            sleep(random.randint(2,10))\n",
    "            \n",
    "            # Obtain parent window handle\n",
    "            parent = driver.window_handles[0]\n",
    "            \n",
    "            # Obtain browser tab window handle\n",
    "            tab = driver.window_handles[1]  \n",
    "            \n",
    "            # Switch to tab browser\n",
    "            driver.switch_to.window(tab)       \n",
    "            \n",
    "            \n",
    "            try:\n",
    "               # Find details button using xpath and click on it\n",
    "                driver.find_element(by='xpath', \n",
    "                                    value = '//*[@id=\"details\"]/div[2]/button/span').click()\n",
    "\n",
    "                # Sleep for a few seconds before extracting details information\n",
    "                sleep(2)\n",
    "                \n",
    "                # Find detail section using id, extract details and append them as text\n",
    "                detail_info.append(driver.find_element(by = 'id', value='details').text)  \n",
    "                \n",
    "                # Update counter for a succesfull scrape\n",
    "                counter += 1\n",
    "                print(f'Scraped {counter} homes')\n",
    "            \n",
    "            except:\n",
    "                # If details page is missing print\n",
    "                print('Details missing')\n",
    "            \n",
    "            # Add random sleep to prevent being blocked by website\n",
    "            sleep(random.randint(2,10))\n",
    "            \n",
    "            # Close tab browser before opening the next one\n",
    "            driver.close()          \n",
    "            \n",
    "            # Switch to parent window and repeat the process for all tags\n",
    "            driver.switch_to.window(parent)    \n",
    "\n",
    "    return basic_info, detail_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f34cdb",
   "metadata": {},
   "source": [
    "## Functions to convert scraped lists into pandas DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2487f566",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------------------\n",
    "# Function to rename duplicated attributes/column names with a number sequence to distinguish\n",
    "#-------------------------------------------------------------------------------------------\n",
    "def rename(df, column):\n",
    "    '''\n",
    "    Renames duplicated column names and adds a sequnce of numbers after to distinguish it\n",
    "    from others\n",
    "    INPUTS\n",
    "    df: dataframe \n",
    "    column: column name which has duplicates to be renamed\n",
    "    \n",
    "    OUTPUT\n",
    "    dataframe column with renamed columns\n",
    "    '''\n",
    "    appendents = (df.groupby(column).cumcount().astype(str).replace('0',''))\n",
    "    return (df[column] + appendents)\n",
    "\n",
    "#-------------------------------------------------------------------------------------------\n",
    "# Function to convert scraped lists into pandas DataFrames\n",
    "#-------------------------------------------------------------------------------------------\n",
    "def listtodataframe(basic_info, detail_info):\n",
    "    '''\n",
    "    Function to convert scraped list of basic_info and detail_info in to respective dataframes\n",
    "    INPUTS\n",
    "    basic_info: basic information list generated from web_scrape function\n",
    "    detail_info: detailed information list generated from web_scrape function\n",
    "    \n",
    "    OUTPUTS\n",
    "    basic_df: dataframe of basic information\n",
    "    details_df: dataframe of details information\n",
    "    '''\n",
    "    #---------------------------------------------------------------------------------------\n",
    "    # Create Basic Dataframe using basic_info\n",
    "    #---------------------------------------------------------------------------------------\n",
    "    # Few tags have an extra attribute of sqft area, remove it to enure consistency of \n",
    "    # column names before merging together as dataframe. For this we will be using\n",
    "    # regex\n",
    "    \n",
    "    # Identify pattern for extra sqft area attribute and replace by ''\n",
    "    pattern = '(\\d+.*)(sqft)\\n'\n",
    "    replace = ''\n",
    "    basic_df =  pd.DataFrame(re.sub(pattern, replace, txt).split('\\n') for txt in basic_info)\n",
    "    \n",
    "    # Rename columns of basic df\n",
    "    column_names = ['Price', 'Beds', 'Baths', 'Address', 'MLS#', 'Tag1', 'Tag2']\n",
    "    basic_df.columns = column_names\n",
    "    \n",
    "    # Split mls# column to keep on the number part\n",
    "    basic_df['MLS#'] = basic_df['MLS#'].apply(lambda x:x.split(':')[1])\n",
    "    \n",
    "    #---------------------------------------------------------------------------------------\n",
    "    # Create Details Dataframe using detail_info\n",
    "    #---------------------------------------------------------------------------------------\n",
    "    details_df = pd.DataFrame()\n",
    "    \n",
    "    for txt in detail_info:\n",
    "        # Create a dummy dataframe for each listings and appedn together to create one\n",
    "        # unified details_df data frame \n",
    "        \n",
    "        # First split on new line\n",
    "        dummy = pd.DataFrame(txt.split('\\n'))\n",
    "        \n",
    "        # Split on ':' and get length of list after split\n",
    "        dummy['Null_Attributes_Flag'] = (dummy[0].apply(lambda x:len(x.split(':'))))\n",
    "        \n",
    "        # Remove all attributes where attribute values are missing\n",
    "        dummy = dummy[dummy['Null_Attributes_Flag']>1]\n",
    "        \n",
    "        # Extract Attributes and Values and store as seperate columns\n",
    "        dummy['Attributes'] = dummy[0].apply(lambda x:x.split(':')[0])\n",
    "        dummy['Values'] = dummy[0].apply(lambda x:x.split(':')[1])\n",
    "        \n",
    "        # Drop irrelevant columns\n",
    "        dummy = dummy.drop(columns=[0,'Null_Attributes_Flag'], axis=1)\n",
    "\n",
    "        # Run rename function to rename duplicated entries\n",
    "        dummy['Attributes'] = rename(dummy, 'Attributes')\n",
    "\n",
    "        # Transpose the dataframe \n",
    "        dummy = dummy.set_index('Attributes').transpose()\n",
    "\n",
    "        # Append to details_df and repeat for all entries in detail_info\n",
    "        details_df = details_df.append(dummy)\n",
    "    \n",
    "    # drop the redundant index in the end\n",
    "    details_df.reset_index(drop=True, inplace=True)   \n",
    "    \n",
    "    return basic_df, details_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4dcfe6",
   "metadata": {},
   "source": [
    "## Functions to Generate Data Stats for DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ca12a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_data_stats(df):\n",
    "    '''\n",
    "    Reads a dataframe and generates its statistics and missing value tables\n",
    "    INPUTS:\n",
    "    df = pandas dataframe\n",
    "\n",
    "    OUTPUTS:\n",
    "    data_stats = basic data statistics of dataframe\n",
    "    sorted_missing_data_stats = sorted missing values statistics table\n",
    "    similar_missing_value_cols = missing values grouped by similar missing percentages\n",
    "    nan_count_rows = count of miising values per row\n",
    "    '''\n",
    "    #---------------------------------------------------------------------------\n",
    "    # Dataframe Statistics\n",
    "    #---------------------------------------------------------------------------\n",
    "    data_stats = df.describe(include='all').transpose()\n",
    "    \n",
    "    #---------------------------------------------------------------------------\n",
    "    # Count Missing Values\n",
    "    #---------------------------------------------------------------------------\n",
    "    missing_data_stats=pd.DataFrame(df.isnull().sum(),columns=['Missing_Values'])\n",
    "    \n",
    "    #---------------------------------------------------------------------------\n",
    "    # Perform an assessment of how much missing data there is in each column of the dataset\n",
    "    #---------------------------------------------------------------------------\n",
    "    missing_data_stats['Missing_Percentage']=(missing_data_stats['Missing_Values']/len(df))*100\n",
    "    sorted_missing_data_stats=missing_data_stats.sort_values(by='Missing_Percentage',ascending=False)\n",
    "    \n",
    "    #---------------------------------------------------------------------------\n",
    "    # Calculate the count of columns missing similar percentage of data\n",
    "    #---------------------------------------------------------------------------\n",
    "    sorted_missing_data_stats['Missing_Percentage_Rounded']=sorted_missing_data_stats['Missing_Percentage'].round()\n",
    "    sorted_missing_data_stats.index.name = 'Feature'\n",
    "    similar_missing_value_cols = (sorted_missing_data_stats.drop('Missing_Percentage',axis=1).groupby(\n",
    "                                                                            by='Missing_Percentage_Rounded').count())\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    # How much data is missing in each row of the dataset?\n",
    "    # Sum all the missing values by rows\n",
    "    #---------------------------------------------------------------------------\n",
    "    nan_count_rows=df.isnull().sum(axis=1).sort_values(ascending=False)\n",
    "    \n",
    "    return data_stats, sorted_missing_data_stats,\\\n",
    "           similar_missing_value_cols,nan_count_rows\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bab0045",
   "metadata": {},
   "source": [
    "# Run Webscraping Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0000fd0",
   "metadata": {},
   "source": [
    "## Run <font color ='green'> *web_scrape* </font> function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cbb98ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping this url now => https://www.remax.ca/ab/calgary-real-estate?pageNumber=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gb/8r1vvmnj7k99ch6mg1hrh9y00000gp/T/ipykernel_38072/1704266477.py:55: DeprecationWarning: find_elements_by_* commands are deprecated. Please use find_elements() instead\n",
      "  card_tags = driver.find_elements_by_class_name('listing-card_listingCard__3SoUb')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 1 homes\n",
      "Scraped 2 homes\n",
      "Scraped 3 homes\n",
      "Scraped 4 homes\n",
      "Scraped 5 homes\n",
      "Scraped 6 homes\n",
      "Scraped 7 homes\n",
      "Scraped 8 homes\n",
      "Scraped 9 homes\n",
      "Scraped 10 homes\n",
      "Scraped 11 homes\n",
      "Scraped 12 homes\n",
      "Scraped 13 homes\n",
      "Scraped 14 homes\n",
      "Scraped 15 homes\n",
      "Scraped 16 homes\n",
      "Scraped 17 homes\n",
      "Details missing\n",
      "Scraped 18 homes\n",
      "Scraped 19 homes\n",
      "Scraping this url now => https://www.remax.ca/ab/calgary-real-estate?pageNumber=2\n",
      "Scraped 20 homes\n",
      "Scraped 21 homes\n",
      "Scraped 22 homes\n",
      "Scraped 23 homes\n",
      "Scraped 24 homes\n",
      "Scraped 25 homes\n",
      "Scraped 26 homes\n",
      "Scraped 27 homes\n",
      "Scraped 28 homes\n",
      "Scraped 29 homes\n",
      "Scraped 30 homes\n",
      "Scraped 31 homes\n",
      "Scraped 32 homes\n",
      "Scraped 33 homes\n",
      "Scraped 34 homes\n",
      "Scraped 35 homes\n",
      "Scraped 36 homes\n",
      "Scraped 37 homes\n",
      "Scraped 38 homes\n",
      "Scraped 39 homes\n",
      "Scraping this url now => https://www.remax.ca/ab/calgary-real-estate?pageNumber=3\n",
      "Scraped 40 homes\n",
      "Scraped 41 homes\n",
      "Scraped 42 homes\n",
      "Scraped 43 homes\n",
      "Scraped 44 homes\n",
      "Scraped 45 homes\n",
      "Scraped 46 homes\n",
      "Scraped 47 homes\n",
      "Scraped 48 homes\n",
      "Scraped 49 homes\n",
      "Scraped 50 homes\n",
      "Scraped 51 homes\n",
      "Scraped 52 homes\n",
      "Scraped 53 homes\n",
      "Scraped 54 homes\n",
      "Scraped 55 homes\n",
      "Scraped 56 homes\n",
      "Scraped 57 homes\n",
      "Scraped 58 homes\n",
      "Scraped 59 homes\n",
      "Scraping this url now => https://www.remax.ca/ab/calgary-real-estate?pageNumber=4\n",
      "Scraped 60 homes\n",
      "Scraped 61 homes\n",
      "Scraped 62 homes\n",
      "Scraped 63 homes\n",
      "Scraped 64 homes\n",
      "Scraped 65 homes\n",
      "Scraped 66 homes\n",
      "Scraped 67 homes\n",
      "Scraped 68 homes\n",
      "Scraped 69 homes\n",
      "Scraped 70 homes\n",
      "Scraped 71 homes\n",
      "Scraped 72 homes\n",
      "Scraped 73 homes\n",
      "Scraped 74 homes\n",
      "Scraped 75 homes\n",
      "Scraped 76 homes\n",
      "Scraped 77 homes\n",
      "Scraped 78 homes\n",
      "Scraped 79 homes\n",
      "Scraping this url now => https://www.remax.ca/ab/calgary-real-estate?pageNumber=5\n",
      "Scraped 80 homes\n",
      "Scraped 81 homes\n",
      "Scraped 82 homes\n",
      "Scraped 83 homes\n",
      "Scraped 84 homes\n",
      "Scraped 85 homes\n",
      "Scraped 86 homes\n",
      "Scraped 87 homes\n",
      "Scraped 88 homes\n",
      "Scraped 89 homes\n",
      "Scraped 90 homes\n",
      "Scraped 91 homes\n",
      "Scraped 92 homes\n",
      "Scraped 93 homes\n",
      "Scraped 94 homes\n",
      "Scraped 95 homes\n",
      "Scraped 96 homes\n",
      "Scraped 97 homes\n",
      "Scraped 98 homes\n",
      "Scraped 99 homes\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/gb/8r1vvmnj7k99ch6mg1hrh9y00000gp/T/ipykernel_38072/661895149.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnum_pages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mbasic_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetail_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweb_scrape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprovince\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_pages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Save scraped lists to files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/gb/8r1vvmnj7k99ch6mg1hrh9y00000gp/T/ipykernel_38072/1704266477.py\u001b[0m in \u001b[0;36mweb_scrape\u001b[0;34m(province, city, num_pages)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;31m# Add random sleep to prevent being blocked by website\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0;31m# Close tab browser before opening the next one\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "province = 'ab'\n",
    "city = 'calgary'\n",
    "num_pages = 70\n",
    "\n",
    "basic_info, detail_info = web_scrape(province, city, num_pages)\n",
    "\n",
    "# Save scraped lists to files \n",
    "with open(city+'_'+province+'_'+'basic_info.txt', 'w') as f:\n",
    "    for item in basic_info:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "        \n",
    "with open(city+'_'+province+'_'+'details_info.txt', 'w') as f:\n",
    "    for item in detail_info:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c71f8e",
   "metadata": {},
   "source": [
    "## Run <font color = 'green' > *listtodataframe* </font> function to create and save pandas DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f70c7d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_df, details_df = listtodataframe(basic_info, detail_info)\n",
    "\n",
    "# Save DataFrames\n",
    "basic_df.to_csv(city+'_'+province+'_'+'basic_df')\n",
    "details_df.to_csv(city+'_'+province+'_'+'detail_df')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a25f1aa",
   "metadata": {},
   "source": [
    "# Perform EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4913824c",
   "metadata": {},
   "source": [
    "## Show first few columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e50a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b80e20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "details_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e5def5",
   "metadata": {},
   "source": [
    "## Merge tables into one using MLS# as common key\n",
    "- Do a left join with basic_df as base table to keep all the basic listing information\n",
    "- Save as remax dataframe\n",
    "- visualize first few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84987c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "remax = pd.merge(basic_df, details_df, how = 'left', left_on='MLS#', right_on='MLS® #')\n",
    "remax.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566b7bfc",
   "metadata": {},
   "source": [
    "## Get dataframe stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4177b58c",
   "metadata": {},
   "source": [
    "### Run <font color = 'green'> *missing_data_stats* </font> Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0fd02f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run missing_data_stats Function\n",
    "data_stats, missing_by_cols, missing_by_bins , missing_by_row =  missing_data_stats(remax)\n",
    "\n",
    "print('\\n\\nDataFrame Statistics')\n",
    "display(data_stats)\n",
    "\n",
    "print('\\n\\nMissing Values Statistics')\n",
    "display(missing_by_cols)\n",
    "\n",
    "print('\\n\\nCount of Columns with Similars Missing Values')\n",
    "display(missing_by_bins)\n",
    "\n",
    "print('\\n\\nCount of Missing Values by Rows')\n",
    "display(missing_by_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c107d8ff",
   "metadata": {},
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60568c4c",
   "metadata": {},
   "source": [
    "### Remove columns which have more than threshold percentage of data missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f478ae0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 70\n",
    "cols_below_threshold = (missing_by_cols[missing_by_cols\n",
    "                          ['Missing_Percentage_Rounded'] > threshold].index)\n",
    "\n",
    "remax.drop(cols_below_threshold, axis=1, inplace =True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849dd734",
   "metadata": {},
   "source": [
    "### Re-Run Missing Data Stats Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fb4594",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Re-Run missing_data_stats Function\n",
    "data_stats, missing_by_cols, missing_by_bins , missing_by_row =  missing_data_stats(remax)\n",
    "\n",
    "print('\\n\\nDataFrame Statistics')\n",
    "display(data_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0c5c87",
   "metadata": {},
   "source": [
    "### Correct Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89170e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim all columns\n",
    "remax = remax.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "\n",
    "# Replac NaNs with empty strings\n",
    "remax['Bedroom'].fillna('', inplace =True)\n",
    "remax['Dining Room'].fillna('', inplace =True)\n",
    "remax['Living Room'].fillna('', inplace =True)\n",
    "remax['Kitchen'].fillna('', inplace =True)\n",
    "\n",
    "# Convert Price to Integer\n",
    "remax['Price'] = remax['Price'].apply(lambda x:int(x.replace('$','').replace(',','')))\n",
    "\n",
    "# Convert date objects to datetime\n",
    "remax['Date Listed'] = pd.to_datetime(remax['Date Listed'])\n",
    "remax['Last Updated'] = pd.to_datetime(remax['Last Updated'])\n",
    "\n",
    "# Drop repeated column \n",
    "remax.drop('MLS® #', axis = 1, inplace = True)\n",
    "\n",
    "# Extract Property Tax Amount from Property Tax attribute\n",
    "remax['Property Tax'] = remax['Property Tax'].replace('N/A',np.NaN)\n",
    "remax['Property Tax Amount'] =  (remax['Property Tax'].apply(lambda x:str(x).split('(')[0].\n",
    "                                                        replace('$','').replace(',','').\n",
    "                                                            strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a9872b",
   "metadata": {},
   "outputs": [],
   "source": [
    "remax.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656a2735",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(t):\n",
    "    return [item for sublist in t for item in sublist]\n",
    "\n",
    "def area_from_dims(table, column):\n",
    "    length = []\n",
    "    width = []\n",
    "\n",
    "    for rows in table[column]:\n",
    "\n",
    "        # Identify pattern for area dimensions and extract values to get the area\n",
    "        dim1 = '(\\d+.*)[xX]'\n",
    "        dim2 = '[xX](.\\d+.*)'\n",
    "\n",
    "        dim11 = '(\\d.*)[\\w]'\n",
    "        dim22 = '(\\d.*)[\\w]'\n",
    "\n",
    "        le = re.findall(dim11, str(re.findall(dim1, rows)))\n",
    "        wi = re.findall(dim22, str(re.findall(dim2, rows)))\n",
    "\n",
    "        if len(le)>0 and len(wi)>0:\n",
    "            length.append(le)\n",
    "            width.append(wi)\n",
    "        else:\n",
    "            length.append([0])\n",
    "            width.append([0])\n",
    "            \n",
    "\n",
    "    length = flatten(length)\n",
    "    width = flatten(width)\n",
    "    \n",
    "    area = []\n",
    "    \n",
    "    for l,w in zip(length,width):\n",
    "        try:\n",
    "            area.append(round(float(l.replace(\"'\",''))*float(w.replace(\"'\",'')), 2))\n",
    "        except:\n",
    "            area.append(0)\n",
    "\n",
    "    return area\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11eb59d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "remax['Bedroom_Area'] = area_from_dims(remax,'Bedroom')\n",
    "remax['Dining_Area'] = area_from_dims(remax,'Dining Room')\n",
    "remax['Living_Room_Area'] = area_from_dims(remax,'Living Room')\n",
    "remax['Kitchen_Area'] = area_from_dims(remax,'Kitchen')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c494a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "remax[['Bedroom','Kitchen','Bedroom_Area', 'Kitchen_Area']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce442ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "remax.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c97eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['Property Tax','Last Updated','Bedroom', 'Dining Room','Living Room',\n",
    "            'Kitchen']\n",
    "remax.drop(drop_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae956ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16,10))\n",
    "sns.histplot(remax['Price'])\n",
    "# plt.hist(remax['Price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9663be37",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16,10))\n",
    "sns.histplot(remax['Bedroom_Area'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e83876",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(remax['Bedroom_Area'], remax['Price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5743c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remax['Bedroom'].fillna('', inplace =True)\n",
    "\n",
    "# # Identify pattern for area dimensions and extract values to get the area\n",
    "# dim1 = '(\\d+.*)[xX]'\n",
    "# dim2 = '[xX](.\\d+.*)'\n",
    "\n",
    "# remax['Bedroom_dim1'] = pd.DataFrame(re.findall(dim1, txt) \n",
    "#                          for txt in remax[remax['Bedroom'].notnull()]['Bedroom'])\n",
    "# remax['Bedroom_dim2'] = pd.DataFrame(re.findall(dim2, txt) \n",
    "#                          for txt in remax[remax['Bedroom'].notnull()]['Bedroom'])\n",
    "\n",
    "# remax['Bedroom_dim1'] = remax['Bedroom_dim1'].fillna('').apply(lambda x:x.split('m')[0].strip())\n",
    "# remax['Bedroom_dim2'] = remax['Bedroom_dim2'].fillna('').apply(lambda x:x.split('m')[0].strip())\n",
    "\n",
    "# remax['Bedroom_dim1'].replace(to_replace='',value=np.nan, inplace =True)\n",
    "# remax['Bedroom_dim2'].replace(to_replace='',value=np.nan, inplace =True)\n",
    "\n",
    "# remax['Bedroom_Area'] = remax['Bedroom_dim1'].astype(float)*remax['Bedroom_dim2'].astype(float)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
